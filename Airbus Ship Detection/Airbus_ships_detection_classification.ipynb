{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Airbus_ships_detection_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEIeIlSp_Owz",
        "colab_type": "text"
      },
      "source": [
        "#Airbus Ship Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqBOKxeh2bIF",
        "colab_type": "text"
      },
      "source": [
        "Following the preprocessing file , we are going to upload from Google Drive, a compressed folder which includes 4070 Images (2035 images with ships and 2035 images without ships) and a csv file which includes images related annotations. A convolutional neural network will be trained to classify images with and without ships and then will be tested. This classifier will be apply furtherly to images before segmentation neural networks to try to improve segmentation results. \n",
        "\n",
        "Even with a reduced number of images, we have got  RAM \"Out of memory errors\" . To fix that problem, original  images shape will be reduced from 768pixels x 768pixels to 256pixels x 256pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5N3JksHrE6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# librairies installation for importing files from google drive and if necessary for unzipping them\n",
        "!pip install -U -q PyDrive\n",
        "!apt-get install unrar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSFrLIiiteoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing basic librairies\n",
        "import os,sys,random\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd \n",
        "from skimage.io import imread,imsave\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import statistics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkyO9vU-tpmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing keras librairies\n",
        "import keras\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import concatenate,Conv2D,Input,MaxPooling2D,UpSampling2D,Dropout,Activation                   \n",
        "from keras.models import Model\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5AuUxV9trM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing librairies for  interaction between Google Collaboratory and  Google Drive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import files\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3Vz1meqE1Jho",
        "colab": {}
      },
      "source": [
        "#images uploading and unzipping\n",
        "!rm -rf airbus_ship_images_4070\n",
        "!mkdir airbus_ship_images_4070\n",
        "downloaded = drive.CreateFile({'id':'1aoZHD_yXnxtuadLeCjso076_ndyAR4Ot'})\n",
        "downloaded.GetContentFile('airbus_ship_images_4070.rar')\n",
        "!unrar e -y *.rar /content/airbus_ship_images_4070/ -idq\n",
        "!rm *.rar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plt-Ug3R29y6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#annotations file uploading\n",
        "downloaded = drive.CreateFile({'id':'13GxdyJ76-LYTFZMOLZDgjZKCHql3qPRu'})\n",
        "downloaded.GetContentFile('new_ships_dataset.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEVLlJaTlCLc",
        "colab_type": "code",
        "outputId": "0c31136c-4252-4c6e-b133-9559ee5d1858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "#reading annotations ('ImageId' and 'ship'features are relevant features for classification)\n",
        "sample_class= pd.read_csv('new_ships_dataset.csv')\n",
        "sample_class.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ImageId</th>\n",
              "      <th>EncodedPixels</th>\n",
              "      <th>ship</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0e3277345.jpg</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3d315bf5c.jpg</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ab4c61871.jpg</td>\n",
              "      <td>31563 1 32330 3 33097 5 33864 7 34631 9 35398 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>f624897e4.jpg</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>581e286b4.jpg</td>\n",
              "      <td>185650 2 186414 6 187179 10 187944 13 188708 1...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         ImageId                                      EncodedPixels  ship\n",
              "0  0e3277345.jpg                                                NaN     0\n",
              "1  3d315bf5c.jpg                                                NaN     0\n",
              "2  ab4c61871.jpg  31563 1 32330 3 33097 5 33864 7 34631 9 35398 ...     1\n",
              "3  f624897e4.jpg                                                NaN     0\n",
              "4  581e286b4.jpg  185650 2 186414 6 187179 10 187944 13 188708 1...     1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "401H7y-FNwJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data splitting -> training 80% and validation 20% \n",
        "# class proportion is maintained with stratify parameter\n",
        "X=sample_class\n",
        "y=sample_class['ship']\n",
        "img_train, img_val, class_train,class_val = train_test_split(X, y,stratify=y,test_size = 0.2,random_state=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdxSPTk8dufH",
        "colab_type": "code",
        "outputId": "836b4f4a-6e9c-4f3d-cfab-3d67f08b033c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(\"% of images without ships in training set is {0}, % of images without ships in validation set is {1}\".format(class_train.value_counts()[0]/len(class_train)*100,class_val.value_counts()[0]/len(class_val)*100))\n",
        "print(\"% of images with    ships in training set is {0}, % of images with    ships in validation set is {1}\".format(class_train.value_counts()[1]/len(class_train)*100,class_val.value_counts()[1]/len(class_val)*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% of images without ships in training set is 50.0, % of images without ships in validation set is 50.0\n",
            "% of images with    ships in training set is 50.0, % of images with    ships in validation set is 50.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5X7e7PmlOyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generation of batches of images and classes('ship' feature)\n",
        "# df dataframe where features are read\n",
        "# new size is the size of the input images used to train the convolutional neural network\n",
        "# batch size is the number of training images in one forward/backward pass through the neural network.\n",
        "def image_batch_generator(df,new_size,batch_size=64):\n",
        "  \n",
        "    batch_img, batch_class = [], []\n",
        "    \n",
        "    list_batches=list(df['ImageId'])\n",
        "  \n",
        "    while True:\n",
        "    \n",
        "        np.random.shuffle(list_batches)\n",
        "      # shuffle is used in case all training images cannot be trained due to RAM out of memory error\n",
        "      # using random batches at the end of each epoch a higher number of images are used to train neural network with a better result \n",
        "    \n",
        "        for img_name in list_batches:\n",
        "            #image resizing to new_size\n",
        "            img_path = os.path.join('airbus_ship_images_4070/',img_name)\n",
        "            img = Image.open(img_path)\n",
        "            img_resized = img.resize((new_size,new_size),Image.ANTIALIAS)\n",
        "            \n",
        "            np_image = np.array(img_resized)\n",
        "      \n",
        "            class_image=df[df['ImageId']==img_name]['ship'].iloc[0]\n",
        "      \n",
        "            np_class = np.array(class_image)\n",
        "            #batch list of images\n",
        "            batch_img.append(np_image)\n",
        "            #batch list of classes\n",
        "            batch_class.append(np_class)\n",
        "      \n",
        "            if len(batch_img)>=batch_size:\n",
        "                # transformation list to array(images and classes) and normalisation of images\n",
        "                # dimension expansion for Keras input matching\n",
        "                yield np.stack(batch_img, axis=0)/255., np.expand_dims(np.stack(batch_class, axis=0),axis=1)\n",
        "                batch_img, batch_class=[], []\n",
        "        # total number of images is not a multiple of batch_size-> residual batch_size lower than all other batches size\n",
        "        if len(batch_img) !=0:\n",
        "            #transformation list to array(images and classes) and normalisation of images\n",
        "            # dimension expansion for Keras input matching\n",
        "            yield np.stack(batch_img, axis=0)/255., np.expand_dims(np.stack(batch_class, axis=0),axis=1)\n",
        "            batch_img, batch_class=[], []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgCSTdgXljPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convolutional neural network\n",
        "def Classifier(height,width,n_channels):\n",
        "    #height->image height\n",
        "    #width ->image width \n",
        "    #n_channels: 1 for grey images and 3 for color images\n",
        "    inputs = (height, width, n_channels)\n",
        "    \n",
        "    classifier=Sequential()\n",
        "    #input convolutional layer 32 filters 3x3, input->(height,width,n_channels)\n",
        "    classifier.add(Conv2D(32,kernel_size=(3,3),input_shape=inputs))\n",
        "    classifier.add(Activation('relu'))\n",
        "    classifier.add(BatchNormalization())#normalisation of weights to avoid overfitting\n",
        "    classifier.add(MaxPooling2D(2,2)) # selection of most significant pixel in a 2x2 filter\n",
        "    #hidden convolutional layer nº1 48 filters 3x3, input->(height/2,width/2,32)\n",
        "    classifier.add(Conv2D(48,kernel_size=(3,3)))\n",
        "    classifier.add(Activation('relu'))\n",
        "    classifier.add(BatchNormalization())#normalisation of weights to avoid overfitting\n",
        "    classifier.add(MaxPooling2D(2,2))\n",
        "    #hidden convolutional layer nº2 64 filters 3x3, input->(height/4,width/4,48)\n",
        "    classifier.add(Conv2D(64,kernel_size=(3,3)))\n",
        "    classifier.add(Activation('relu'))\n",
        "    classifier.add(BatchNormalization())#normalisation of weights to avoid overfitting\n",
        "    classifier.add(MaxPooling2D(2,2))\n",
        "    #hidden convolutional layer nº3 96 filters 3x3, input->(height/8,width/8,64)\n",
        "    classifier.add(Conv2D(96,kernel_size=(3,3)))\n",
        "    classifier.add(Activation('relu'))\n",
        "    classifier.add(BatchNormalization())#normalisation of weights to avoid overfitting\n",
        "    classifier.add(MaxPooling2D(2,2))\n",
        "    #hidden convolutional layer nº4 128 filters 3x3, input->(height/16,width/16,96)\n",
        "    classifier.add(Conv2D(128,kernel_size=(3,3)))\n",
        "    classifier.add(Activation('relu'))\n",
        "    classifier.add(BatchNormalization())#normalisation of weights to avoid overfitting\n",
        "    classifier.add(MaxPooling2D(2,2))\n",
        "    #output array of convolutional layers is flattened into a vector of 1 dimension, input -> (height/32,width/32,128) \n",
        "    classifier.add(Flatten())\n",
        "    #hidden dense layer 128 neurons, input -> (height/32*width/32*128,)\n",
        "    classifier.add(Dense(units=128)) \n",
        "    classifier.add(BatchNormalization())#normalisation of weights to avoid overfitting\n",
        "    classifier.add(Dropout(0.3))#random deactivation of 30% of neurons to avoid overfitting\n",
        "    #output dense layer 1 neuron, input-> (128,)\n",
        "    classifier.add(Dense(units=1))\n",
        "    classifier.add(BatchNormalization())#normalisation of weights to avoid overfitting\n",
        "    classifier.add(Activation('sigmoid'))\n",
        "    \n",
        "    return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEio_4WXllON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#classifier model creation with 256x256 resized coloured images\n",
        "classifier=Classifier(256,256,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q4vGIYhlqdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model compilation with a cross_entropy loss function and accuracy metrics\n",
        "classifier.compile(optimizer=Adam(1e-3, decay=2e-6),loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k5C1ch_MSv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#as a small amount of images is used to train neural network, at each batch,several operations are randomly performed on images\n",
        "#these operations are rotation,shifting,zooming and flipping and they augment artificially the number of different images\n",
        "data_gen_args = dict(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "Img_datagen = ImageDataGenerator(**data_gen_args)\n",
        "Class_datagen = ImageDataGenerator()\n",
        "\n",
        "def augmentation_generator(img_batch_gen,img_datagen,class_datagen):\n",
        "  \n",
        "  for in_img, in_class in img_batch_gen:\n",
        "    seed = np.random.choice(range(1000))\n",
        "    #images are denormalize to apply augmentation operations\n",
        "    aug_img = img_datagen.flow(255*in_img,in_class,batch_size = in_img.shape[0],seed=seed,shuffle=True)\n",
        "   \n",
        "    x, y = next(aug_img)\n",
        "    #images are normalized again to be applied to neural network\n",
        "    yield x/255.0, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYiuIBuCly_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#parameters of model fit function\n",
        "#train generator has the following shape (48,256,256,3) 48 coloured images 256pixels x 256pixels \n",
        "train_generator=augmentation_generator(image_batch_generator(img_train, 256,batch_size=48),Img_datagen,Class_datagen)\n",
        "val_generator=next(image_batch_generator(img_val,256,batch_size=len(img_val)))\n",
        "steps_train=len(img_train)//48"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6TSUQj7WTEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#callback functions: \n",
        "#saving best weights, save metrics and losses in file classification_weights.best.hdf5\n",
        "#learning rate is reduce at 90% if result accuracy is not improved in 3 epochs\n",
        "#training is stopped if result accuracy is not improved after 40 epochs\n",
        "weight_path=\"{}_weights.best.hdf5\".format('classification')\n",
        "\n",
        "checkpoint = ModelCheckpoint(weight_path, monitor='val_acc', verbose=1, \n",
        "                             save_best_only=True, mode='max', save_weights_only = True)\n",
        "\n",
        "reduceLROnPlat = ReduceLROnPlateau(monitor='val_acc', factor=0.9, \n",
        "                                   patience=3, \n",
        "                                   verbose=1, mode='max', epsilon=0.0001, cooldown=2, min_lr=1e-6)\n",
        "early = EarlyStopping(monitor=\"val_acc\", \n",
        "                      mode=\"max\", \n",
        "                      patience=40) # probably needs to be more patient, but kaggle time is limited\n",
        "\n",
        "callbacks_classification = [checkpoint, early, reduceLROnPlat]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEqcbxtmmAFu",
        "colab_type": "code",
        "outputId": "bd211d28-864d-4e0a-c5be-7c12bd1461f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6502
        }
      },
      "source": [
        "#training the model\n",
        "loss_classifier=classifier.fit_generator(train_generator, validation_data=val_generator, steps_per_epoch=steps_train, epochs=100,workers=1,callbacks=callbacks_classification)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "67/67 [==============================] - 112s 2s/step - loss: 0.4845 - acc: 0.7914 - val_loss: 0.3956 - val_acc: 0.8354\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.83538, saving model to classification_weights.best.hdf5\n",
            "Epoch 2/100\n",
            "67/67 [==============================] - 103s 2s/step - loss: 0.4453 - acc: 0.8259 - val_loss: 0.4170 - val_acc: 0.8231\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.83538\n",
            "Epoch 3/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.4221 - acc: 0.8455 - val_loss: 0.4176 - val_acc: 0.8342\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.83538\n",
            "Epoch 4/100\n",
            "67/67 [==============================] - 108s 2s/step - loss: 0.4082 - acc: 0.8548 - val_loss: 0.4102 - val_acc: 0.8378\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.83538 to 0.83784, saving model to classification_weights.best.hdf5\n",
            "Epoch 5/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.3981 - acc: 0.8641 - val_loss: 0.5168 - val_acc: 0.7101\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.83784\n",
            "Epoch 6/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.3844 - acc: 0.8794 - val_loss: 0.3809 - val_acc: 0.8403\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.83784 to 0.84029, saving model to classification_weights.best.hdf5\n",
            "Epoch 7/100\n",
            "67/67 [==============================] - 108s 2s/step - loss: 0.3695 - acc: 0.8792 - val_loss: 0.3606 - val_acc: 0.8514\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.84029 to 0.85135, saving model to classification_weights.best.hdf5\n",
            "Epoch 8/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.3608 - acc: 0.8826 - val_loss: 0.4893 - val_acc: 0.7703\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.85135\n",
            "Epoch 9/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.3564 - acc: 0.8858 - val_loss: 0.3160 - val_acc: 0.8907\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.85135 to 0.89066, saving model to classification_weights.best.hdf5\n",
            "Epoch 10/100\n",
            "67/67 [==============================] - 108s 2s/step - loss: 0.3419 - acc: 0.8865 - val_loss: 0.3469 - val_acc: 0.8845\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.89066\n",
            "Epoch 11/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.3361 - acc: 0.8823 - val_loss: 0.3276 - val_acc: 0.8919\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.89066 to 0.89189, saving model to classification_weights.best.hdf5\n",
            "Epoch 12/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.3305 - acc: 0.8891 - val_loss: 0.2969 - val_acc: 0.9042\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.89189 to 0.90418, saving model to classification_weights.best.hdf5\n",
            "Epoch 13/100\n",
            "67/67 [==============================] - 107s 2s/step - loss: 0.3204 - acc: 0.8965 - val_loss: 0.3153 - val_acc: 0.8821\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.90418\n",
            "Epoch 14/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.3068 - acc: 0.9063 - val_loss: 0.3765 - val_acc: 0.8710\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.90418\n",
            "Epoch 15/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.3038 - acc: 0.9027 - val_loss: 0.3127 - val_acc: 0.8919\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.90418\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
            "Epoch 16/100\n",
            "67/67 [==============================] - 106s 2s/step - loss: 0.2962 - acc: 0.9052 - val_loss: 0.3257 - val_acc: 0.8919\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.90418\n",
            "Epoch 17/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.2957 - acc: 0.9083 - val_loss: 0.3068 - val_acc: 0.8931\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.90418\n",
            "Epoch 18/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.2902 - acc: 0.9092 - val_loss: 0.2660 - val_acc: 0.8980\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.90418\n",
            "Epoch 19/100\n",
            "67/67 [==============================] - 106s 2s/step - loss: 0.2871 - acc: 0.9071 - val_loss: 0.2722 - val_acc: 0.9079\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.90418 to 0.90786, saving model to classification_weights.best.hdf5\n",
            "Epoch 20/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.2795 - acc: 0.9064 - val_loss: 0.3463 - val_acc: 0.8563\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.90786\n",
            "Epoch 21/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.2729 - acc: 0.9088 - val_loss: 0.3226 - val_acc: 0.8636\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.90786\n",
            "Epoch 22/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.2768 - acc: 0.9103 - val_loss: 0.3086 - val_acc: 0.8796\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.90786\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
            "Epoch 23/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.2572 - acc: 0.9178 - val_loss: 0.2931 - val_acc: 0.9214\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.90786 to 0.92138, saving model to classification_weights.best.hdf5\n",
            "Epoch 24/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.2540 - acc: 0.9187 - val_loss: 0.2849 - val_acc: 0.8870\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.92138\n",
            "Epoch 25/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.2604 - acc: 0.9241 - val_loss: 0.2825 - val_acc: 0.9066\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.92138\n",
            "Epoch 26/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.2458 - acc: 0.9229 - val_loss: 0.2755 - val_acc: 0.9128\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.92138\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
            "Epoch 27/100\n",
            "67/67 [==============================] - 106s 2s/step - loss: 0.2572 - acc: 0.9180 - val_loss: 0.3134 - val_acc: 0.8808\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.92138\n",
            "Epoch 28/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.2432 - acc: 0.9218 - val_loss: 0.2782 - val_acc: 0.8796\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.92138\n",
            "Epoch 29/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.2435 - acc: 0.9221 - val_loss: 0.2638 - val_acc: 0.8894\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.92138\n",
            "Epoch 30/100\n",
            "67/67 [==============================] - 108s 2s/step - loss: 0.2476 - acc: 0.9174 - val_loss: 0.2760 - val_acc: 0.8968\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.92138\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
            "Epoch 31/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.2237 - acc: 0.9340 - val_loss: 0.2625 - val_acc: 0.9115\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.92138\n",
            "Epoch 32/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.2356 - acc: 0.9220 - val_loss: 0.2345 - val_acc: 0.9201\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.92138\n",
            "Epoch 33/100\n",
            "67/67 [==============================] - 106s 2s/step - loss: 0.2219 - acc: 0.9300 - val_loss: 0.2295 - val_acc: 0.9300\n",
            "\n",
            "Epoch 00033: val_acc improved from 0.92138 to 0.92998, saving model to classification_weights.best.hdf5\n",
            "Epoch 34/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.2242 - acc: 0.9279 - val_loss: 0.2571 - val_acc: 0.9189\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.92998\n",
            "Epoch 35/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.2203 - acc: 0.9335 - val_loss: 0.2262 - val_acc: 0.9189\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.92998\n",
            "Epoch 36/100\n",
            "67/67 [==============================] - 108s 2s/step - loss: 0.2104 - acc: 0.9388 - val_loss: 0.2185 - val_acc: 0.9287\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.92998\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
            "Epoch 37/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.2139 - acc: 0.9305 - val_loss: 0.2431 - val_acc: 0.9140\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.92998\n",
            "Epoch 38/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.2047 - acc: 0.9370 - val_loss: 0.2419 - val_acc: 0.9054\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.92998\n",
            "Epoch 39/100\n",
            "67/67 [==============================] - 107s 2s/step - loss: 0.2114 - acc: 0.9302 - val_loss: 0.2418 - val_acc: 0.9189\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.92998\n",
            "Epoch 40/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.1982 - acc: 0.9396 - val_loss: 0.2300 - val_acc: 0.9189\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.92998\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0005314410547725857.\n",
            "Epoch 41/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.1961 - acc: 0.9406 - val_loss: 0.2232 - val_acc: 0.9177\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.92998\n",
            "Epoch 42/100\n",
            "67/67 [==============================] - 107s 2s/step - loss: 0.1981 - acc: 0.9415 - val_loss: 0.2836 - val_acc: 0.8857\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.92998\n",
            "Epoch 43/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1976 - acc: 0.9407 - val_loss: 0.2068 - val_acc: 0.9238\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.92998\n",
            "Epoch 44/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.1978 - acc: 0.9385 - val_loss: 0.2225 - val_acc: 0.9140\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.92998\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00047829695977270604.\n",
            "Epoch 45/100\n",
            "67/67 [==============================] - 106s 2s/step - loss: 0.1965 - acc: 0.9381 - val_loss: 0.2446 - val_acc: 0.9115\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.92998\n",
            "Epoch 46/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1776 - acc: 0.9498 - val_loss: 0.2205 - val_acc: 0.9201\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.92998\n",
            "Epoch 47/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.1831 - acc: 0.9471 - val_loss: 0.2094 - val_acc: 0.9201\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.92998\n",
            "Epoch 48/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1804 - acc: 0.9468 - val_loss: 0.1955 - val_acc: 0.9361\n",
            "\n",
            "Epoch 00048: val_acc improved from 0.92998 to 0.93612, saving model to classification_weights.best.hdf5\n",
            "Epoch 49/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1862 - acc: 0.9452 - val_loss: 0.2261 - val_acc: 0.9263\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.93612\n",
            "Epoch 50/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.1792 - acc: 0.9453 - val_loss: 0.2329 - val_acc: 0.9177\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.93612\n",
            "Epoch 51/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1815 - acc: 0.9423 - val_loss: 0.2356 - val_acc: 0.9103\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.93612\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0004304672533180565.\n",
            "Epoch 52/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1768 - acc: 0.9449 - val_loss: 0.2126 - val_acc: 0.9201\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.93612\n",
            "Epoch 53/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1790 - acc: 0.9461 - val_loss: 0.2579 - val_acc: 0.9079\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.93612\n",
            "Epoch 54/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1724 - acc: 0.9462 - val_loss: 0.2342 - val_acc: 0.9226\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.93612\n",
            "Epoch 55/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.1683 - acc: 0.9457 - val_loss: 0.2098 - val_acc: 0.9251\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.93612\n",
            "\n",
            "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.00038742052274756136.\n",
            "Epoch 56/100\n",
            "67/67 [==============================] - 106s 2s/step - loss: 0.1712 - acc: 0.9454 - val_loss: 0.1927 - val_acc: 0.9263\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.93612\n",
            "Epoch 57/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1614 - acc: 0.9548 - val_loss: 0.2813 - val_acc: 0.8968\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.93612\n",
            "Epoch 58/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.1708 - acc: 0.9464 - val_loss: 0.2295 - val_acc: 0.9103\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.93612\n",
            "Epoch 59/100\n",
            "67/67 [==============================] - 106s 2s/step - loss: 0.1529 - acc: 0.9576 - val_loss: 0.2418 - val_acc: 0.9140\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.93612\n",
            "\n",
            "Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.0003486784757114947.\n",
            "Epoch 60/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1661 - acc: 0.9517 - val_loss: 0.1942 - val_acc: 0.9312\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.93612\n",
            "Epoch 61/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.1604 - acc: 0.9560 - val_loss: 0.2058 - val_acc: 0.9201\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.93612\n",
            "Epoch 62/100\n",
            "67/67 [==============================] - 106s 2s/step - loss: 0.1633 - acc: 0.9499 - val_loss: 0.2033 - val_acc: 0.9312\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.93612\n",
            "Epoch 63/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1569 - acc: 0.9526 - val_loss: 0.2090 - val_acc: 0.9263\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.93612\n",
            "\n",
            "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.00031381062290165574.\n",
            "Epoch 64/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.1490 - acc: 0.9558 - val_loss: 0.2133 - val_acc: 0.9189\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.93612\n",
            "Epoch 65/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1540 - acc: 0.9563 - val_loss: 0.2077 - val_acc: 0.9201\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.93612\n",
            "Epoch 66/100\n",
            "67/67 [==============================] - 107s 2s/step - loss: 0.1460 - acc: 0.9573 - val_loss: 0.2060 - val_acc: 0.9251\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.93612\n",
            "Epoch 67/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.1490 - acc: 0.9565 - val_loss: 0.1964 - val_acc: 0.9312\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.93612\n",
            "\n",
            "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.0002824295632308349.\n",
            "Epoch 68/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1486 - acc: 0.9583 - val_loss: 0.2011 - val_acc: 0.9251\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.93612\n",
            "Epoch 69/100\n",
            "67/67 [==============================] - 107s 2s/step - loss: 0.1409 - acc: 0.9605 - val_loss: 0.2081 - val_acc: 0.9201\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.93612\n",
            "Epoch 70/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.1353 - acc: 0.9611 - val_loss: 0.2127 - val_acc: 0.9263\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.93612\n",
            "Epoch 71/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1424 - acc: 0.9623 - val_loss: 0.1978 - val_acc: 0.9214\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.93612\n",
            "\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.00025418660952709616.\n",
            "Epoch 72/100\n",
            "67/67 [==============================] - 107s 2s/step - loss: 0.1404 - acc: 0.9567 - val_loss: 0.2088 - val_acc: 0.9201\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.93612\n",
            "Epoch 73/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.1376 - acc: 0.9626 - val_loss: 0.1987 - val_acc: 0.9152\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.93612\n",
            "Epoch 74/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.1312 - acc: 0.9644 - val_loss: 0.1932 - val_acc: 0.9238\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.93612\n",
            "Epoch 75/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1378 - acc: 0.9588 - val_loss: 0.2071 - val_acc: 0.9300\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.93612\n",
            "\n",
            "Epoch 00075: ReduceLROnPlateau reducing learning rate to 0.00022876793809700757.\n",
            "Epoch 76/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1325 - acc: 0.9620 - val_loss: 0.2019 - val_acc: 0.9238\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.93612\n",
            "Epoch 77/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.1343 - acc: 0.9608 - val_loss: 0.1993 - val_acc: 0.9263\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.93612\n",
            "Epoch 78/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1329 - acc: 0.9645 - val_loss: 0.2036 - val_acc: 0.9238\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.93612\n",
            "Epoch 79/100\n",
            "67/67 [==============================] - 106s 2s/step - loss: 0.1270 - acc: 0.9637 - val_loss: 0.2062 - val_acc: 0.9238\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.93612\n",
            "\n",
            "Epoch 00079: ReduceLROnPlateau reducing learning rate to 0.00020589114428730683.\n",
            "Epoch 80/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.1295 - acc: 0.9668 - val_loss: 0.1934 - val_acc: 0.9300\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.93612\n",
            "Epoch 81/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1296 - acc: 0.9648 - val_loss: 0.1826 - val_acc: 0.9287\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.93612\n",
            "Epoch 82/100\n",
            "67/67 [==============================] - 106s 2s/step - loss: 0.1226 - acc: 0.9685 - val_loss: 0.1951 - val_acc: 0.9263\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.93612\n",
            "Epoch 83/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.1305 - acc: 0.9633 - val_loss: 0.1799 - val_acc: 0.9324\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.93612\n",
            "\n",
            "Epoch 00083: ReduceLROnPlateau reducing learning rate to 0.00018530203378759326.\n",
            "Epoch 84/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1231 - acc: 0.9660 - val_loss: 0.1932 - val_acc: 0.9300\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.93612\n",
            "Epoch 85/100\n",
            "67/67 [==============================] - 106s 2s/step - loss: 0.1177 - acc: 0.9698 - val_loss: 0.1933 - val_acc: 0.9337\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.93612\n",
            "Epoch 86/100\n",
            "67/67 [==============================] - 104s 2s/step - loss: 0.1194 - acc: 0.9673 - val_loss: 0.2000 - val_acc: 0.9300\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.93612\n",
            "Epoch 87/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1263 - acc: 0.9642 - val_loss: 0.1972 - val_acc: 0.9226\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.93612\n",
            "\n",
            "Epoch 00087: ReduceLROnPlateau reducing learning rate to 0.00016677183302817866.\n",
            "Epoch 88/100\n",
            "67/67 [==============================] - 105s 2s/step - loss: 0.1163 - acc: 0.9703 - val_loss: 0.1906 - val_acc: 0.9263\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.93612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0PIE7YTTi_h",
        "colab_type": "text"
      },
      "source": [
        "We obtain a 93.61% best value accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nzW_17pJEqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#best weight values are downloaded to PC\n",
        "files.download('classification_weights.best.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ-Tz3PeOGz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#previous best weights are loaded into the classifier model\n",
        "classifier.load_weights('classification_weights.best.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-AapU7gTM58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generation of output images batches to make predictions\n",
        "val_batch=image_batch_generator(img_val,256,len(img_val))\n",
        "val_batch_image,val_batch_class=next(val_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQteDzJwVuFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prediction of validation set images\n",
        "#as classifier outputs are probabilities threshold=0.5 is considered to generate 1s and 0s.\n",
        "y_pred=classifier.predict(val_batch_image)\n",
        "y_pred=np.where(y_pred >= 0.5, 1,0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXk74v-FWKdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm=confusion_matrix(y_pred,val_batch_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXPHreTLciEV",
        "colab_type": "code",
        "outputId": "f2b5f85a-4734-4878-d258-1d6e880789a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "#confusion matrix plot\n",
        "labels=[0,1]\n",
        "plt.figure(figsize=(6,3))\n",
        "sns.heatmap(cm,annot=True,cmap=\"YlGnBu\",xticklabels=labels,yticklabels=labels,fmt='g')\n",
        "plt.xlabel('True Class')\n",
        "plt.ylabel('Predicted Class')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAADTCAYAAACldCmtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x216 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuSslpkRTxAM",
        "colab_type": "text"
      },
      "source": [
        "We can check with this confusion matrix the best accuracy obtained during training time. Accuracy is (True positive->371) + (True negative->391) divided by ((True positive->371) + (True negative ->391)+(False positive->16)+(False negative->36))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXl0LQpDTywF",
        "colab_type": "code",
        "outputId": "a680376f-1492-4494-98f1-6bc081f21cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Accuracy -> {:.2f}%\".format((371+391)/(371+391+16+36)*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy -> 93.61%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdLVXmDfVbth",
        "colab_type": "text"
      },
      "source": [
        "All classification model weights have been saved and are ready to be loaded to preclassify images before segmentation. If improvement is achieved, classification and segmentation will be used sequentially."
      ]
    }
  ]
}